import requests
import cv2
import numpy as np
from ultralytics import YOLO
import time
import os
from gtts import gTTS
from playsound import playsound

# =============================
# Ø±Ø§Ø¨Ø· ESP32-CAM
# =============================
CAMERA_URL = "http://192.168.12.90/capture"

# =============================
# ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø£Ø´ÙŠØ§Ø¡
# =============================
translate = {
    "person": "Ø´Ø®Øµ",
    "chair": "ÙƒØ±Ø³ÙŠ",
    "table": "Ø·Ø§ÙˆÙ„Ø©",
    "bottle": "Ø²Ø¬Ø§Ø¬Ø©",
    "cup": "ÙƒÙˆØ¨",
    "cell phone": "Ù‡Ø§ØªÙ",
    "laptop": "Ø­Ø§Ø³ÙˆØ¨ Ù…Ø­Ù…ÙˆÙ„",
    "tv": "ØªÙ„ÙØ§Ø²",
    "book": "ÙƒØªØ§Ø¨",
    "backpack": "Ø­Ù‚ÙŠØ¨Ø©",
    "door": "Ø¨Ø§Ø¨",
    "window": "Ù†Ø§ÙØ°Ø©",
    "computer": "Ø­Ø§Ø³ÙˆØ¨",
    "keyboard": "Ù„ÙˆØ­Ø© Ù…ÙØ§ØªÙŠØ­",
    "bench": "Ù…Ù‚Ø¹Ø¯",
    "trash can": "Ø³Ù„Ø© Ù…Ù‡Ù…Ù„Ø§Øª"
}

# =============================
# Ù†Ø·Ù‚ Ø¹Ø±Ø¨ÙŠ (Google)
# =============================
def speak(text):
    tts = gTTS(text=text, lang="ar")
    tts.save("voice.mp3")
    playsound("voice.mp3")
    os.remove("voice.mp3")

# =============================
# ØªØ­Ù…ÙŠÙ„ YOLO
# =============================
model = YOLO("yolov8n.pt")

last_sentence = ""

print("ğŸŸ¢ Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ¹Ù…Ù„")
speak("Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ¹Ù…Ù„")

# =============================
# ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
# =============================
def get_direction(x_center, width):
    if x_center < width / 3:
        return "Ø¹Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø±"
    elif x_center > 2 * width / 3:
        return "Ø¹Ù„Ù‰ Ø§Ù„ÙŠÙ…ÙŠÙ†"
    else:
        return "Ø£Ù…Ø§Ù…Ùƒ"

# =============================
# Ù‚Ø±ÙŠØ¨ / Ø¨Ø¹ÙŠØ¯
# =============================
def get_distance(box_area, frame_area):
    ratio = box_area / frame_area
    if ratio > 0.15:
        return "Ù‚Ø±ÙŠØ¨"
    else:
        return "Ø¨Ø¹ÙŠØ¯"

# =============================
# Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# =============================
while True:
    try:
        r = requests.get(CAMERA_URL, timeout=5)
        img = cv2.imdecode(np.frombuffer(r.content, np.uint8), cv2.IMREAD_COLOR)

        h, w, _ = img.shape
        frame_area = h * w

        results = model(img, verbose=False)
        descriptions = []

        for res in results:
            for box in res.boxes:
                cls_id = int(box.cls[0])
                eng = model.names[cls_id]
                ar = translate.get(eng, eng)

                x1, y1, x2, y2 = map(int, box.xyxy[0])
                x_center = (x1 + x2) // 2
                area = (x2 - x1) * (y2 - y1)

                direction = get_direction(x_center, w)
                distance = get_distance(area, frame_area)

                desc = f"{ar} {distance} {direction}"
                descriptions.append(desc)

        if descriptions:
            sentence = "Ø£Ù…Ø§Ù…Ùƒ " + " Ùˆ ".join(descriptions)
        else:
            sentence = "Ù„Ø§ Ø£Ø±Ù‰ Ø´ÙŠØ¡ ÙˆØ§Ø¶Ø­"

        # ğŸ” Ù„Ø§ ØªÙƒØ±Ø± Ø§Ù„ÙƒÙ„Ø§Ù…
        if sentence != last_sentence:
            print(sentence)
            speak(sentence)
            last_sentence = sentence

        cv2.imshow("ESP32-CAM", img)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

        time.sleep(3)

    except Exception as e:
        print("âŒ Ø®Ø·Ø£:", e)
        speak("Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„")
        time.sleep(5)

cv2.destroyAllWindows()
